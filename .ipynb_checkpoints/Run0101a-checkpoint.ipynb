{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to debug the Run0101 series.\n",
    "\n",
    "MISSION:\n",
    "Keep branch1, branch2 datasets together. Otherwise wrong matching!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code base starts from Run0101 and modifies it thereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Late Fusion Module (test) - Functional API\n",
    "'''\n",
    "\n",
    "# Multiple Inputs\n",
    "import keras\n",
    "from keras.optimizers import RMSprop, adam, Adam\n",
    "from keras.initializers import TruncatedNormal, glorot_normal\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import pandas\n",
    "import numpy\n",
    "import sys\n",
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new Metric: rmse = Root Mean Square Error\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square( y_true-y_pred )))\n",
    "\n",
    "# Define custon LateFusionActivation function\n",
    "def custom_activation(LateFusionActivation):\n",
    "    return TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the current file name. Useful for procedurally generating output/log files.\n",
    "file_name =  os.path.basename(sys.argv[0][:-3])\n",
    "\n",
    "# Define neural network parameters\n",
    "batch_size = 10\n",
    "#num_classes = 1\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'shuffled_clean_input_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-8de18771258e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;31m# Creates training dataset (90% of total data) for inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m x_train = shuffled_clean_input_df_7D.iloc[:int(\n\u001b[1;32m---> 57\u001b[1;33m     numpy.round(len(shuffled_clean_input_df)*0.9))]\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;31m# Creates testing dataset (10% of total data) for outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'shuffled_clean_input_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Data (which is in HDF5 or .h5 format)\n",
    "store = pandas.HDFStore(\"unstable_training_gen3_7D_nions0_flat_filter8.h5\")\n",
    "target_df = store['/output/efeETG_GB'].to_frame()  # This one is relatively easy to train\n",
    "input_df = store['input']\n",
    "\n",
    "# Puts inputs and outputs in the same pandas dataframe.\n",
    "# Also only keeps overlapping entries.\n",
    "joined_dataFrame = target_df.join(input_df)\n",
    "\n",
    "# Make a copy of joined_dataFrame for later use\n",
    "joined_dataFrame_original = deepcopy(joined_dataFrame)\n",
    "\n",
    "\n",
    "'''\n",
    "# Make a copy of joined_dataFrame for branch1\n",
    "joined_dataFrame_1 = deepcopy(joined_dataFrame)\n",
    "\n",
    "# Make a copy of joined_dataFrame for branch2\n",
    "joined_dataFrame_2 = deepcopy(joined_dataFrame)\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "# *************************************************************************** #\n",
    "# Normalize data by standard deviation and mean-centering the data\n",
    "# Standard configuration\n",
    "joined_dataFrame['efeETG_GB'] = (joined_dataFrame['efeETG_GB'] - joined_dataFrame['efeETG_GB'].mean()) / joined_dataFrame['efeETG_GB'].std()\n",
    "joined_dataFrame['Ati'] = (joined_dataFrame['Ati'] - joined_dataFrame['Ati'].mean()) / joined_dataFrame['Ati'].std()\n",
    "joined_dataFrame['Ate'] = (joined_dataFrame['Ate'] - joined_dataFrame['Ate'].mean()) / joined_dataFrame['Ate'].std()\n",
    "joined_dataFrame['An'] = (joined_dataFrame['An'] - joined_dataFrame['An'].mean()) / joined_dataFrame['An'].std()\n",
    "joined_dataFrame['q'] = (joined_dataFrame['q'] - joined_dataFrame['q'].mean()) / joined_dataFrame['q'].std()\n",
    "joined_dataFrame['smag'] = (joined_dataFrame['smag'] - joined_dataFrame['smag'].mean()) / joined_dataFrame['smag'].std()\n",
    "joined_dataFrame['x'] = (joined_dataFrame['x'] - joined_dataFrame['x'].mean()) / joined_dataFrame['x'].std()\n",
    "joined_dataFrame['Ti_Te'] = (joined_dataFrame['Ti_Te'] - joined_dataFrame['Ti_Te'].mean()) / joined_dataFrame['Ti_Te'].std()\n",
    "\n",
    "# Shuffles dataset\n",
    "shuffled_joined_dataFrame = joined_dataFrame.reindex(numpy.random.permutation(\n",
    "                                                joined_dataFrame.index))\n",
    "\n",
    "# Creates a pandas dataframe for the outputs\n",
    "shuffled_clean_output_df = shuffled_joined_dataFrame['efeETG_GB']\n",
    "\n",
    "shuffled_joined_dataFrame_base = deepcopy(shuffled_joined_dataFrame)\n",
    "\n",
    "\n",
    "\n",
    "# *************************************************************************** #\n",
    "# Creates a pandas dataframe for the inputs (7D)\n",
    "shuffled_clean_input_df_7D = shuffled_joined_dataFrame.drop('efeETG_GB', axis=1)\n",
    "\n",
    "# Creates training dataset (90% of total data) for outputs\n",
    "y_train = shuffled_clean_output_df.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_output_df)*0.9))]\n",
    "\n",
    "# Creates training dataset (90% of total data) for inputs\n",
    "x_train = shuffled_clean_input_df_7D.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_input_df)*0.9))]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for outputs\n",
    "y_test = shuffled_clean_output_df.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_output_df)*0.9)):]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for inputs\n",
    "x_test = shuffled_clean_input_df_7D.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_input_df)*0.9)):]\n",
    "# *************************************************************************** #\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "# *************************************************************************** #\n",
    "# Normalize data by standard deviation and mean-centering the data\n",
    "# Inputs for branch1\n",
    "joined_dataFrame_1['efeETG_GB'] = (joined_dataFrame_1['efeETG_GB'] - joined_dataFrame_1['efeETG_GB'].mean()) / joined_dataFrame_1['efeETG_GB'].std()\n",
    "joined_dataFrame_1['Ati'] = (joined_dataFrame_1['Ati'] - joined_dataFrame_1['Ati'].mean()) / joined_dataFrame_1['Ati'].std()\n",
    "# joined_dataFrame_1['Ate'] = (joined_dataFrame_1['Ate'] - joined_dataFrame_1['Ate'].mean()) / joined_dataFrame_1['Ate'].std()\n",
    "joined_dataFrame_1['An'] = (joined_dataFrame_1['An'] - joined_dataFrame_1['An'].mean()) / joined_dataFrame_1['An'].std()\n",
    "joined_dataFrame_1['q'] = (joined_dataFrame_1['q'] - joined_dataFrame_1['q'].mean()) / joined_dataFrame_1['q'].std()\n",
    "joined_dataFrame_1['smag'] = (joined_dataFrame_1['smag'] - joined_dataFrame_1['smag'].mean()) / joined_dataFrame_1['smag'].std()\n",
    "joined_dataFrame_1['x'] = (joined_dataFrame_1['x'] - joined_dataFrame_1['x'].mean()) / joined_dataFrame_1['x'].std()\n",
    "joined_dataFrame_1['Ti_Te'] = (joined_dataFrame_1['Ti_Te'] - joined_dataFrame_1['Ti_Te'].mean()) / joined_dataFrame_1['Ti_Te'].std()\n",
    "\n",
    "print(\"joined_dataFrame_1.shape\")\n",
    "print(joined_dataFrame_1.shape)\n",
    "\n",
    "# Shuffles dataset\n",
    "shuffled_joined_dataFrame_1 = joined_dataFrame_1.reindex(numpy.random.permutation(\n",
    "                                                joined_dataFrame_1.index))\n",
    "\n",
    "# Creates a pandas dataframe for the outputs\n",
    "shuffled_clean_output_df_1 = shuffled_joined_dataFrame_1['efeETG_GB']\n",
    "\n",
    "print(\"shuffled_clean_output_df_1.shape\")\n",
    "print(shuffled_clean_output_df_1.shape)\n",
    "\n",
    "# Creates a pandas dataframe for the inputs\n",
    "shuffled_clean_input_df_1 = shuffled_joined_dataFrame_1.drop('efeETG_GB', axis=1)\n",
    "\n",
    "print(\"shuffled_clean_input_df_1.shape\")\n",
    "print(shuffled_clean_input_df_1.shape)\n",
    "\n",
    "# Creates training dataset (90% of total data) for outputs\n",
    "y_train_1 = shuffled_clean_output_df_1.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_output_df_1)*0.9))]\n",
    "\n",
    "# Creates training dataset (90% of total data) for inputs\n",
    "x_train_1 = shuffled_clean_input_df_1.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_input_df_1)*0.9))]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for outputs\n",
    "y_test_1 = shuffled_clean_output_df_1.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_output_df_1)*0.9)):]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for inputs\n",
    "x_test_1 = shuffled_clean_input_df_1.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_input_df_1)*0.9)):]\n",
    "# *************************************************************************** #\n",
    "\n",
    "# *************************************************************************** #\n",
    "# Normalize data by standard deviation and mean-centering the data\n",
    "# Inputs for branch2\n",
    "joined_dataFrame_2['Ate'] = (joined_dataFrame_2['Ate'] - joined_dataFrame_2['Ate'].mean()) / joined_dataFrame_2['Ate'].std()\n",
    "\n",
    "# Shuffles dataset\n",
    "shuffled_joined_dataFrame_2 = joined_dataFrame_2.reindex(numpy.random.permutation(\n",
    "                                                joined_dataFrame_2.index))\n",
    "\n",
    "# Creates a pandas dataframe for the outputs\n",
    "shuffled_clean_output_df_2 = shuffled_joined_dataFrame_2['efeETG_GB']\n",
    "\n",
    "# Creates a pandas dataframe for the inputs\n",
    "shuffled_clean_input_df_2 = shuffled_joined_dataFrame_2.drop('efeETG_GB', axis=1)\n",
    "\n",
    "# Creates training dataset (90% of total data) for outputs\n",
    "y_train_2 = shuffled_clean_output_df_2.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_output_df_2)*0.9))]\n",
    "\n",
    "# Creates training dataset (90% of total data) for inputs\n",
    "x_train_2 = shuffled_clean_input_df_2.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_input_df_2)*0.9))]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for outputs\n",
    "y_test_2 = shuffled_clean_output_df_2.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_output_df_2)*0.9)):]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for inputs\n",
    "x_test_2 = shuffled_clean_input_df_2.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_input_df_2)*0.9)):]\n",
    "# *************************************************************************** #\n",
    "\n",
    "# Deletes pandas dataframes that are no longer needed\n",
    "del target_df, input_df\n",
    "\n",
    "# Closes the HDFStore. This is good practice.\n",
    "store.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(x_test_1.shape)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_clean_input_df_7D.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# branch1\n",
    "visible_branch1 = Input(shape=(6,))\n",
    "hidden1_branch1 = Dense(30)(visible_branch1)\n",
    "hidden2_branch1 = Dense(30)(hidden1_branch1)\n",
    "\n",
    "# branch2\n",
    "visible_branch2 = Input(shape=(1,))\n",
    "\n",
    "# merge input models\n",
    "merge = concatenate([hidden2_branch1, visible_branch2])\n",
    "\n",
    "# interpretation model\n",
    "output = Dense(1, activation='sigmoid')(merge)\n",
    "\n",
    "model = Model(inputs=[visible_branch1, visible_branch2], outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "print(model.summary())\n",
    "\n",
    "# plot graph\n",
    "# plot_model(model, 'ModelPlots/' + str(file_name) + 'model_plot.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='mean_squared_error',   #categorical_crossentropy\n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=[\"mae\", \"mean_squared_error\", rmse])\n",
    "\n",
    "# Add CallBacks (including TensorBoard)\n",
    "tbCallBack = keras.callbacks.TensorBoard(\n",
    "        log_dir='TensorBoard_logs/' + str(file_name), write_graph = False, write_images=False, write_grads=False)\n",
    "EarlyStoppingCallBack = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_rmse', min_delta=0, patience=15, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit([x_train_1, x_train_2],\n",
    "                    y = y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    validation_data=(x_test, y_test),\n",
    "                    # validation_data=([x_test_branch1, x_test_branch2], y_test),\n",
    "                    callbacks=[tbCallBack, EarlyStoppingCallBack])\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('val_mean_absolute_error:', score[1])\n",
    "\n",
    "print(\"score\")\n",
    "print(score)\n",
    "\n",
    "print(\"model.metrics_names\")\n",
    "print(model.metrics_names)\n",
    "\n",
    "# creates a HDF5 file 'my_model.h5'\n",
    "model.save(\"./Saved-Networks/\" + str(file_name) +\".h5\")\n",
    "\n",
    "# Create output file\n",
    "OutputFile = open(\"./Loss-Values/\" +str(file_name) +\".txt\", \"w+\")\n",
    "OutputFile.write(\"Test loss: \" + str(score[0]) + \"\\n\")\n",
    "OutputFile.write(\"val_mean_absolute_error: \" +str(score[1]) + \"\\n\")\n",
    "OutputFile.write(\"val_mean_squared_error: \" +str(score[2]) + \"\\n\")\n",
    "OutputFile.write(\"RMSE: \" +str(score[3]) + \"\\n\")\n",
    "OutputFile.close()\n",
    "\n",
    "del history\n",
    "del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
