{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Objective:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run0130x-SERIES\n",
    "\n",
    "I took `Run0131` and added Karel's suggestions for the power layer.\n",
    "\n",
    "-  `Run0141-Mk3`\n",
    "\n",
    "- Exactly the same as `Run0142` but now with the TensorBoard callback recording more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT RUN THIS PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Gets the current file name. Useful for procedurally generating output/log files.\n",
    "file_name =  os.path.basename(sys.argv[0][:-3])\n",
    "print(file_name)\n",
    "\n",
    "if file_name == \"ipykernel_launcher\":\n",
    "    print(\"This is the Jupyter version.\")\n",
    "    print(\"Now MANUALLY run the next two cells!\")\n",
    "    print(\"STOP! This should not be in your code!!\")\n",
    "    exit(0)\n",
    "    time.sleep(10)\n",
    "    print(\"Testing if script has really stopped.\")\n",
    "else:\n",
    "    print(\"This is the Atom version\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RUN ONLY IN JUPYTER!!\n",
    "# Start here (manual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.kernel.execute('file_name = \"' + IPython.notebook.notebook_name + '\"');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = file_name[:-6]\n",
    "print(file_name)\n",
    "\n",
    "is_Jupyter = True\n",
    "print(is_Jupyter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code for both ATOM & JUPYTER from now (Run all cells below now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Late Fusion Module (test) - Functional API\n",
    "'''\n",
    "\n",
    "# Multiple Inputs\n",
    "import keras\n",
    "from keras.optimizers import RMSprop, adam, Adam\n",
    "from keras.initializers import TruncatedNormal, glorot_normal, Constant\n",
    "from keras.utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import MaxoutDense\n",
    "from keras.layers.merge import concatenate\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "#from keras.backend import switch\n",
    "import pandas\n",
    "import numpy\n",
    "import sys\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.layers.advanced_activations import ThresholdedReLU\n",
    "\n",
    "#keras.backend.clear_session()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define new Metric: rmse = Root Mean Square Error\n",
    "def rmse(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square( y_true-y_pred )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify for ATOM use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_Jupyter == True:\n",
    "    pass\n",
    "else:\n",
    "    # Gets the current file name. Useful for procedurally generating output/log files.\n",
    "    file_name =  os.path.basename(sys.argv[0][:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network parameters\n",
    "batch_size = 10\n",
    "#num_classes = 1\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data (which is in HDF5 or .h5 format)\n",
    "store = pandas.HDFStore(\"training_gen3_7D_nions0_flat_filter8.h5\")\n",
    "target_df = store['/output/efeETG_GB'].to_frame()  # This one is relatively easy to train\n",
    "input_df = store['input']\n",
    "\n",
    "# Puts inputs and outputs in the same pandas dataframe.\n",
    "# Also only keeps overlapping entries.\n",
    "joined_dataFrame = target_df.join(input_df)\n",
    "\n",
    "# Make a copy of joined_dataFrame for later use\n",
    "joined_dataFrame_original = deepcopy(joined_dataFrame)\n",
    "\n",
    "\n",
    "# *************************************************************************** #\n",
    "# Normalize data by standard deviation and mean-centering the data\n",
    "# Standard configuration\n",
    "joined_dataFrame['efeETG_GB'] = (joined_dataFrame['efeETG_GB'] - joined_dataFrame['efeETG_GB'].mean()) / joined_dataFrame['efeETG_GB'].std()\n",
    "joined_dataFrame['Ati'] = (joined_dataFrame['Ati'] - joined_dataFrame['Ati'].mean()) / joined_dataFrame['Ati'].std()\n",
    "joined_dataFrame['Ate'] = (joined_dataFrame['Ate'] - joined_dataFrame['Ate'].mean()) / joined_dataFrame['Ate'].std()\n",
    "joined_dataFrame['An'] = (joined_dataFrame['An'] - joined_dataFrame['An'].mean()) / joined_dataFrame['An'].std()\n",
    "joined_dataFrame['q'] = (joined_dataFrame['q'] - joined_dataFrame['q'].mean()) / joined_dataFrame['q'].std()\n",
    "joined_dataFrame['smag'] = (joined_dataFrame['smag'] - joined_dataFrame['smag'].mean()) / joined_dataFrame['smag'].std()\n",
    "joined_dataFrame['x'] = (joined_dataFrame['x'] - joined_dataFrame['x'].mean()) / joined_dataFrame['x'].std()\n",
    "joined_dataFrame['Ti_Te'] = (joined_dataFrame['Ti_Te'] - joined_dataFrame['Ti_Te'].mean()) / joined_dataFrame['Ti_Te'].std()\n",
    "\n",
    "# Shuffles dataset\n",
    "shuffled_joined_dataFrame = joined_dataFrame.reindex(numpy.random.permutation(\n",
    "                                                joined_dataFrame.index))\n",
    "\n",
    "# Creates a pandas dataframe for the outputs\n",
    "shuffled_clean_output_df = shuffled_joined_dataFrame['efeETG_GB']\n",
    "\n",
    "# Make a copy of shuffled_joined_dataFrame for later use\n",
    "shuffled_joined_dataFrame_base = deepcopy(shuffled_joined_dataFrame)\n",
    "\n",
    "\n",
    "\n",
    "# *************************************************************************** #\n",
    "# Creates a pandas dataframe for the inputs (7D)\n",
    "shuffled_clean_input_df_7D = shuffled_joined_dataFrame.drop('efeETG_GB', axis=1)\n",
    "\n",
    "# Creates training dataset (90% of total data) for outputs\n",
    "y_train = shuffled_clean_output_df.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_output_df)*0.9))]\n",
    "\n",
    "# Creates training dataset (90% of total data) for inputs\n",
    "x_train = shuffled_clean_input_df_7D.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_input_df_7D)*0.9))]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for outputs\n",
    "y_test = shuffled_clean_output_df.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_output_df)*0.9)):]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for inputs\n",
    "x_test = shuffled_clean_input_df_7D.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_input_df_7D)*0.9)):]\n",
    "# *************************************************************************** #\n",
    "\n",
    "\n",
    "# Deletes pandas dataframes that are no longer needed\n",
    "del target_df, input_df\n",
    "\n",
    "# Closes the HDFStore. This is good practice.\n",
    "store.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_clean_input_df_7D.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a pandas dataframe for the inputs\n",
    "shuffled_clean_input_df_1 = shuffled_clean_input_df_7D.drop('Ate', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_clean_input_df_1.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shuffled_clean_input_df_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_clean_input_df_2 = shuffled_clean_input_df_7D.drop('Ati', axis=1)\n",
    "shuffled_clean_input_df_2 = shuffled_clean_input_df_2.drop('An', axis=1)\n",
    "shuffled_clean_input_df_2 = shuffled_clean_input_df_2.drop('q', axis=1)\n",
    "shuffled_clean_input_df_2 = shuffled_clean_input_df_2.drop('smag', axis=1)\n",
    "shuffled_clean_input_df_2 = shuffled_clean_input_df_2.drop('x', axis=1)\n",
    "shuffled_clean_input_df_2 = shuffled_clean_input_df_2.drop('Ti_Te', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_clean_input_df_2.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shuffled_clean_input_df_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************************************************************** #\n",
    "# Branch 1\n",
    "\n",
    "# Creates training dataset (90% of total data) for inputs\n",
    "x_train_1 = shuffled_clean_input_df_1.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_input_df_1)*0.9))]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for inputs\n",
    "x_test_1 = shuffled_clean_input_df_1.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_input_df_1)*0.9)):]\n",
    "# *************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************************************************************************** #\n",
    "# Branch 2\n",
    "\n",
    "# Creates training dataset (90% of total data) for inputs\n",
    "x_train_2 = shuffled_clean_input_df_2.iloc[:int(\n",
    "    numpy.round(len(shuffled_clean_input_df_2)*0.9))]\n",
    "\n",
    "# Creates testing dataset (10% of total data) for inputs\n",
    "x_test_2 = shuffled_clean_input_df_2.iloc[int(\n",
    "    numpy.round(len(shuffled_clean_input_df_2)*0.9)):]\n",
    "# *************************************************************************** #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[7].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[3].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_layer(\"c_3_branch3_feeder\").get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_layer(\"c_3_branch3_feeder\").get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_layer(\"c_3_branch3_feeder\").get_weights()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.get_layer(\"c_3_branch3_feeder\").get_weights()[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[14] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.layers[14].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def power_function(x):\n",
    "    #from keras import backend as K\n",
    "    x = model.get_layer(\"c_3_branch3_feeder\").get_weights()[0]\n",
    "    c_3 = K.cast(x, dtype='float32')\n",
    "    if (c_3 >= 0.0):\n",
    "        return c_3\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "    return 1.0 ** c_3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def power_function(x):\n",
    "    #from keras import backend as K\n",
    "    x = model.get_layer(\"c_3_branch3_feeder\").get_weights()[0]\n",
    "    c_3 = K.cast(x, dtype='float32')\n",
    "    #if (c_3 >= 0.0):\n",
    "    #    pass\n",
    "    #else:\n",
    "    #    c_3 = 0.0\n",
    "    #\n",
    "    return 1.0 ** c_3\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def power_function(x):\n",
    "    print(\"counter = 0\")\n",
    "    print(\"x: \")\n",
    "    print(x)\n",
    "    print(\"type(x): \")\n",
    "    print(type(x))\n",
    "    counter = 1\n",
    "    if (counter == 0):\n",
    "        pass\n",
    "    else:\n",
    "        print(\"counter = 1\")\n",
    "        print(\"x: \")\n",
    "        print(x)\n",
    "        print(\"type(x): \")\n",
    "        print(type(x))\n",
    "        a = K.cast(x, dtype='float32')\n",
    "        print(\"a: \")\n",
    "        print(a)\n",
    "        try:\n",
    "            print(model.layers[14])\n",
    "            c_3 = model.layers[14].get_weights()[0][0][0]\n",
    "            print(c_3)\n",
    "            print(\"type(c_3): \")\n",
    "            print(type(c_3))\n",
    "            if (c_3 >= 0.0):\n",
    "                pass\n",
    "            else:\n",
    "                c_3 = 0.0\n",
    "            pooling = 1.0 ** c_3\n",
    "            power_pooling_output = K.cast(pooling, dtype='float32')\n",
    "        except:\n",
    "            print(\"An exception occurred\")\n",
    "    return power_pooling_output\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.add(Lambda(lambda x: 1 ** x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# branch1\n",
    "visible_branch1 = Input(shape=(6, ), name=\"6D_INPUTS\")\n",
    "hidden1_branch1 = Dense(30,\n",
    "        activation='tanh',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=regularizers.l2(0.00005),\n",
    "        use_bias=True, bias_initializer='glorot_normal',\n",
    "        name=\"hidden1_branch1\")(visible_branch1)\n",
    "hidden2_branch1 = Dense(30,\n",
    "        activation='tanh',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=regularizers.l2(0.00005),\n",
    "        use_bias=True, bias_initializer='glorot_normal',\n",
    "        name=\"hidden2_branch1\")(hidden1_branch1)\n",
    "theta_branch1 = Dense(1,\n",
    "        activation='tanh',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=regularizers.l2(0.00005),\n",
    "        use_bias=True, bias_initializer='glorot_normal',\n",
    "        name=\"theta_branch1\")(hidden2_branch1)\n",
    "theta_branch1_feeder = Dense(1,\n",
    "        activation='linear',\n",
    "        kernel_initializer=Constant(value=-1),\n",
    "        bias_initializer='Zeros',\n",
    "        trainable=False,\n",
    "        name=\"theta_branch1_feeder\")(theta_branch1)\n",
    "\n",
    "# branch2 (Ate input)\n",
    "visible_branch2 = Input(shape=(1, ), name=\"Ate_INPUT\")\n",
    "visible_branch2_feeder = Dense(1,\n",
    "                activation='linear',\n",
    "                name=\"visible_branch2_feeder\")(visible_branch2)\n",
    "\n",
    "# addition_pooling (effectively subtraction though...)\n",
    "addition_pooling = keras.layers.Add(name=\"Addition_Pooling\")([theta_branch1_feeder, visible_branch2_feeder])\n",
    "\n",
    "TR = Dense(1, activation='relu',\n",
    "           kernel_initializer='Ones',\n",
    "           bias_initializer='Zeros',\n",
    "           trainable=False,\n",
    "           name=\"TR\")(addition_pooling)\n",
    "\n",
    "# branch 3 (for c_3)\n",
    "hidden1_branch3 = Dense(30,\n",
    "        activation='tanh',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=regularizers.l2(0.00005),\n",
    "        use_bias=True, bias_initializer='glorot_normal',\n",
    "        name=\"hidden1_branch3\")(visible_branch1)\n",
    "hidden2_branch3 = Dense(30,\n",
    "        activation='tanh',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=regularizers.l2(0.00005),\n",
    "        use_bias=True, bias_initializer='glorot_normal',\n",
    "        name=\"hidden2_branch3\")(hidden1_branch3)\n",
    "c_3_branch3 = Dense(1,\n",
    "        activation='tanh',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=regularizers.l2(0.00005),\n",
    "        use_bias=True, bias_initializer='glorot_normal',\n",
    "        name=\"c_3_branch3\")(hidden2_branch3)\n",
    "c_3_branch3_feeder = Dense(1,\n",
    "        kernel_initializer=Constant(value=+1),\n",
    "        bias_initializer='Zeros',\n",
    "        trainable=False,\n",
    "        name=\"c_3_branch3_feeder\")(c_3_branch3)\n",
    "\n",
    "# Power_Pooling\n",
    "#merge = concatenate([TR, c_3_branch3_feeder], name=\"merge\")\n",
    "#power_pooling = Lambda(power_function, name=\"Power_Pooling\")(merge)\n",
    "#power_pooling = Lambda(lambda x: 1 ** x)(c_3_branch3_feeder)\n",
    "\n",
    "\n",
    "power_layer = Lambda(lambda x: K.clip(K.abs(x[0]), 1.0, 10) ** K.abs(x[1]))\n",
    "power_pooling = power_layer([TR, c_3_branch3_feeder])\n",
    "\n",
    "\n",
    "# branch 4 (for the gradient)\n",
    "hidden1_branch4 = Dense(30,\n",
    "        activation='tanh',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=regularizers.l2(0.00005),\n",
    "        use_bias=True, bias_initializer='glorot_normal',\n",
    "        name=\"hidden1_branch4\")(visible_branch1)\n",
    "hidden2_branch4 = Dense(30,\n",
    "        activation='tanh',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=regularizers.l2(0.00005),\n",
    "        use_bias=True, bias_initializer='glorot_normal',\n",
    "        name=\"hidden2_branch4\")(hidden1_branch4)\n",
    "m_branch4 = Dense(1,\n",
    "        activation='tanh',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=regularizers.l2(0.00005),\n",
    "        use_bias=True, bias_initializer='glorot_normal',\n",
    "        name=\"m_branch4\")(hidden2_branch4)\n",
    "m_branch4_feeder = Dense(1,\n",
    "        activation='linear',\n",
    "        kernel_initializer=Constant(value=+1),\n",
    "        bias_initializer='Zeros',\n",
    "        trainable=False,\n",
    "        name=\"m_branch4_feeder\")(m_branch4)\n",
    "\n",
    "# multiplication pooling\n",
    "multiplication_pooling = keras.layers.Multiply(name=\"Multiplication_Pooling\")([m_branch4_feeder, power_pooling])\n",
    "\n",
    "# interpretation model\n",
    "output = Dense(1, activation='linear',\n",
    "           kernel_initializer='Ones',\n",
    "           bias_initializer='Zeros',\n",
    "           trainable=True,\n",
    "           name=\"Output_Layer\")(multiplication_pooling)\n",
    "\n",
    "model = Model(inputs=[visible_branch1, visible_branch2], outputs=output)\n",
    "\n",
    "# summarize layers\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph\n",
    "plot_model(model, 'ModelPlots/' + str(file_name) + '_model_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',   #categorical_crossentropy\n",
    "              optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999),\n",
    "              metrics=[\"mae\", \"mean_squared_error\", rmse])\n",
    "\n",
    "# Add CallBacks (including TensorBoard)\n",
    "tbCallBack = keras.callbacks.TensorBoard(\n",
    "        log_dir='TensorBoard_logs/' + str(file_name), histogram_freq=1, write_graph = False, write_images=False, write_grads=True)\n",
    "EarlyStoppingCallBack = keras.callbacks.EarlyStopping(\n",
    "        monitor='val_rmse', min_delta=0, patience=15, verbose=0, mode='auto')\n",
    "\n",
    "history = model.fit([x_train_1, x_train_2],\n",
    "                    y = y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    shuffle=True,\n",
    "                    verbose=2,\n",
    "                    # validation_data=(x_test, y_test),\n",
    "                    validation_data=([x_test_1, x_test_2], y_test),\n",
    "                    callbacks=[tbCallBack, EarlyStoppingCallBack])\n",
    "\n",
    "# score = model.evaluate(x_test, y_test, verbose=0)\n",
    "score = model.evaluate([x_test_1, x_test_2], y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('val_mean_absolute_error:', score[1])\n",
    "\n",
    "print(\"score\")\n",
    "print(score)\n",
    "\n",
    "print(\"model.metrics_names\")\n",
    "print(model.metrics_names)\n",
    "\n",
    "# creates a HDF5 file 'my_model.h5'\n",
    "model.save(\"./Saved-Networks/\" + str(file_name) +\".h5\")\n",
    "\n",
    "# Create output file\n",
    "OutputFile = open(\"./Loss-Values/\" +str(file_name) +\".txt\", \"w+\")\n",
    "OutputFile.write(\"Test loss: \" + str(score[0]) + \"\\n\")\n",
    "OutputFile.write(\"val_mean_absolute_error: \" +str(score[1]) + \"\\n\")\n",
    "OutputFile.write(\"val_mean_squared_error: \" +str(score[2]) + \"\\n\")\n",
    "OutputFile.write(\"RMSE: \" +str(score[3]) + \"\\n\")\n",
    "OutputFile.close()\n",
    "\n",
    "del history\n",
    "del model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
